{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nasa_news():\n",
    "    '''\n",
    "    creates a 2 item list of the title and description \n",
    "    of the latest nasa news article\n",
    "    returns [title, summary]\n",
    "    '''\n",
    "    # Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text.\n",
    "    # https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\n",
    "    # makes the request and the soup\n",
    "    url_nasa_news = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "    response = requests.get(url_nasa_news)\n",
    "    nasa_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # finds and extracts the target data\n",
    "    nasa_news_title_raw = nasa_soup.find('div', class_='content_title').text\n",
    "    nasa_news_desc_raw = nasa_soup.find('div', class_='slide').find('div', class_='rollover_description').text\n",
    "    \n",
    "    # cleans the strings\n",
    "    nasa_news_title = nasa_news_title_raw.replace('\\n', '')\n",
    "    nasa_news_desc = nasa_news_desc_raw.replace('\\n', '')\n",
    "    \n",
    "    # returns the list\n",
    "    return [nasa_news_title, nasa_news_desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_featured_img():\n",
    "    '''\n",
    "    returns the url of the featured space img and the title\n",
    "    [title, url]\n",
    "    '''\n",
    "    # scrape and save the featured image\n",
    "    # https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\n",
    "    # make request and soup\n",
    "    url_featured_img = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    response = requests.get(url_featured_img)\n",
    "    img_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # builds the url to find the img\n",
    "    url_img_part_a = 'https://www.jpl.nasa.gov'\n",
    "    url_img_part_b = img_soup.find('div', class_='carousel_items').a['data-fancybox-href']\n",
    "    featured_img_url = url_img_part_a + url_img_part_b\n",
    "    \n",
    "    # saves the img title\n",
    "    raw_img_title = img_soup.find('div', class_='carousel_items').h1.text\n",
    "    img_title = raw_img_title.replace('\\r','').replace('\\n','').replace('\\t','').strip()\n",
    "    \n",
    "    return [img_title, featured_img_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # splinter browser activation\n",
    "    executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_twiter_mars_weather():\n",
    "    # collect the most recent tweet from the Mars Weather twitter account\n",
    "    # https://twitter.com/marswxreport?lang=en\n",
    "    # use splinter\n",
    "    url_mars_weather = 'https://twitter.com/marswxreport?lang=en'\n",
    "    # splinter browser bit\n",
    "    browser = init_browser()\n",
    "    browser.visit(url_mars_weather)\n",
    "    time.sleep(2)\n",
    "    weather_html = browser.html\n",
    "    weather_soup = BeautifulSoup(weather_html, \"html.parser\")\n",
    "    browser.quit()\n",
    "    \n",
    "    print('Done with the browser')\n",
    "    mars_weather = weather_soup.body.div.main.article.find('div', lang='en').text\n",
    "    \n",
    "    return mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_mars_facts():\n",
    "    # Collect and convert the html table from\n",
    "    # https://space-facts.com/mars/\n",
    "    url_mars_facts = 'https://space-facts.com/mars/'\n",
    "    response = requests.get(url_mars_facts)\n",
    "    facts_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # pull the table\n",
    "    facts_table_list = facts_soup.table.find_all('td')\n",
    "    \n",
    "    # each td in the list is on both sides\n",
    "    # description -> data -> description.\n",
    "    # they alternate back and forth\n",
    "    fact_text_list = []\n",
    "    for fact in facts_table_list:\n",
    "        fact_text_list.append(fact.text)\n",
    "        \n",
    "    # split apart the titels and data\n",
    "    x = 0\n",
    "    fact_title = []\n",
    "    fact_data = []\n",
    "    for text in fact_text_list:\n",
    "        if (x%2 == 0):\n",
    "            fact_title.append(text)\n",
    "            x+=1\n",
    "        else:\n",
    "            fact_data.append(text)\n",
    "            x+=1\n",
    "    # make a dict of the table using the titels as keys\n",
    "    # and the data as the value\n",
    "    dict_fact_table = {}\n",
    "    for i in range(len(fact_title)):\n",
    "        dict_fact_table[fact_title[i]] = fact_data[i]\n",
    "    \n",
    "    df = pd.DataFrame(dict_fact_table, index=[0])\n",
    "    html_table = df.to_html()\n",
    "    \n",
    "    return html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_hemi_imgs():\n",
    "    # Obtain a hq img url for each of mars's 4 hemmispheres\n",
    "    # https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\n",
    "    url_mars_hemi = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "    response = requests.get(url_mars_hemi)\n",
    "    hemi_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    hemi_a_tags = hemi_soup.body.find('div', class_ = 'container').find_all('a')\n",
    "    \n",
    "    hemi_img_base_urls = []\n",
    "    hemi_img_titels = []\n",
    "    for tag in hemi_a_tags:\n",
    "        hemi_img_base_urls.append(tag['href'])\n",
    "        hemi_img_titels.append(tag.text)\n",
    "    hemi_img_base_urls.pop(0)\n",
    "    hemi_img_titels.pop(0)\n",
    "    \n",
    "    hemi_final_urls = []\n",
    "    base_hemi_url = 'https://astrogeology.usgs.gov'\n",
    "    for target in hemi_img_base_urls:\n",
    "        url = base_hemi_url + target\n",
    "        response = requests.get(url)\n",
    "        hemi_img_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        hemi_final_urls.append(hemi_img_soup.body.find('div', class_ = 'container').find('div', class_ = 'downloads').a['href'])\n",
    "    \n",
    "    hemisphere_image_urls = []\n",
    "    for x in range(len(hemi_img_titels)):\n",
    "        dit = {}\n",
    "        dit['title'] = hemi_img_titels[x]\n",
    "        dit['img_url'] = hemi_final_urls[x]\n",
    "        hemisphere_image_urls.append(dit)\n",
    "    \n",
    "    return hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
